# SREå¹³å°éƒ¨ç½²å®è·µæŒ‡å—

## ç›®å½•

- [1. å¿«é€Ÿå¼€å§‹](#1-å¿«é€Ÿå¼€å§‹)
- [2. Kubernetesé›†ç¾¤æ­å»º](#2-kubernetesé›†ç¾¤æ­å»º)
- [3. å¯è§‚æµ‹æ€§æ ˆéƒ¨ç½²](#3-å¯è§‚æµ‹æ€§æ ˆéƒ¨ç½²)
- [4. å‘Šè­¦ç³»ç»Ÿé…ç½®](#4-å‘Šè­¦ç³»ç»Ÿé…ç½®)
- [5. GitOpsæµæ°´çº¿](#5-gitopsæµæ°´çº¿)
- [6. æœåŠ¡æ¥å…¥æŒ‡å—](#6-æœåŠ¡æ¥å…¥æŒ‡å—)
- [7. æ•…éšœæ’æŸ¥](#7-æ•…éšœæ’æŸ¥)

---

## 1. å¿«é€Ÿå¼€å§‹

### 1.1 å‰ç½®æ¡ä»¶

```bash
# å¿…éœ€å·¥å…·
- kubectl >= 1.30
- helm >= 3.14
- docker >= 24.0
- terraform >= 1.7 (å¯é€‰)
- argocd CLI >= 2.10

# èµ„æºè¦æ±‚ï¼ˆæœ€å°é…ç½®ï¼‰
- 3ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹ï¼š
  - CPU: 8 cores
  - Memory: 32GB
  - Storage: 500GB SSD
```

### 1.2 ä¸€é”®éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy-sre-platform.sh

set -e

echo "ğŸš€ Starting SRE Platform Deployment..."

# 1. éªŒè¯å‰ç½®æ¡ä»¶
echo "âœ… Checking prerequisites..."
command -v kubectl >/dev/null 2>&1 || { echo "kubectl not found"; exit 1; }
command -v helm >/dev/null 2>&1 || { echo "helm not found"; exit 1; }

# 2. æ·»åŠ Helmä»“åº“
echo "ğŸ“¦ Adding Helm repositories..."
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add argo https://argoproj.github.io/argo-helm
helm repo add istio https://istio-release.storage.googleapis.com/charts
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# 3. åˆ›å»ºå‘½åç©ºé—´
echo "ğŸ“ Creating namespaces..."
kubectl create namespace observability --dry-run=client -o yaml | kubectl apply -f -
kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
kubectl create namespace istio-system --dry-run=client -o yaml | kubectl apply -f -

# 4. éƒ¨ç½²Istio
echo "ğŸŒ Deploying Istio..."
helm install istio-base istio/base -n istio-system --wait
helm install istiod istio/istiod -n istio-system --wait
helm install istio-ingress istio/gateway -n istio-system --wait

# 5. éƒ¨ç½²Prometheus Stack
echo "ğŸ“Š Deploying Prometheus Stack..."
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  -n observability \
  -f config/prometheus-values.yaml \
  --wait --timeout 10m

# 6. éƒ¨ç½²VictoriaMetrics
echo "ğŸ’¾ Deploying VictoriaMetrics..."
helm install victoria-metrics-cluster prometheus-community/victoria-metrics-cluster \
  -n observability \
  -f config/victoriametrics-values.yaml \
  --wait

# 7. éƒ¨ç½²Loki
echo "ğŸ“ Deploying Loki..."
helm install loki grafana/loki-stack \
  -n observability \
  -f config/loki-values.yaml \
  --wait

# 8. éƒ¨ç½²Tempo
echo "ğŸ” Deploying Tempo..."
helm install tempo grafana/tempo-distributed \
  -n observability \
  -f config/tempo-values.yaml \
  --wait

# 9. éƒ¨ç½²ArgoCD
echo "ğŸ”„ Deploying ArgoCD..."
helm install argocd argo/argo-cd \
  -n argocd \
  -f config/argocd-values.yaml \
  --wait

# 10. éƒ¨ç½²SRE Platformåº”ç”¨
echo "ğŸ¯ Deploying SRE Platform..."
kubectl apply -f manifests/sre-platform/ -n observability

echo "âœ… SRE Platform deployed successfully!"
echo ""
echo "ğŸ“ Access points:"
echo "  Grafana: kubectl port-forward -n observability svc/kube-prometheus-stack-grafana 3000:80"
echo "  ArgoCD: kubectl port-forward -n argocd svc/argocd-server 8080:443"
echo ""
echo "ğŸ”‘ Grafana credentials:"
echo "  Username: admin"
echo "  Password: $(kubectl get secret -n observability kube-prometheus-stack-grafana -o jsonpath='{.data.admin-password}' | base64 -d)"
echo ""
echo "ğŸ”‘ ArgoCD credentials:"
echo "  Username: admin"
echo "  Password: $(kubectl get secret -n argocd argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d)"
```

---

## 2. Kubernetesé›†ç¾¤æ­å»º

### 2.1 ä½¿ç”¨kubeadmæ­å»ºHAé›†ç¾¤

```bash
#!/bin/bash
# init-k8s-cluster.sh

# åœ¨æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œ
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# å®‰è£…å®¹å™¨è¿è¡Œæ—¶ï¼ˆcontainerdï¼‰
sudo apt-get update
sudo apt-get install -y containerd

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

# å®‰è£…kubeadmã€kubeletã€kubectl
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
  sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update
sudo apt-get install -y kubelet=1.30.0-1.1 kubeadm=1.30.0-1.1 kubectl=1.30.0-1.1
sudo apt-mark hold kubelet kubeadm kubectl

# åœ¨ç¬¬ä¸€ä¸ªmasterèŠ‚ç‚¹åˆå§‹åŒ–é›†ç¾¤
sudo kubeadm init --control-plane-endpoint="load-balancer:6443" \
  --upload-certs \
  --pod-network-cidr=10.244.0.0/16 \
  --service-cidr=10.96.0.0/12

# é…ç½®kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# å®‰è£…CNIæ’ä»¶ï¼ˆCiliumï¼‰
helm repo add cilium https://helm.cilium.io/
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set operator.replicas=1

# å…¶ä»–masterèŠ‚ç‚¹åŠ å…¥
# ä½¿ç”¨kubeadm initè¾“å‡ºçš„å‘½ä»¤ï¼Œç±»ä¼¼ï¼š
# sudo kubeadm join load-balancer:6443 --token xxx \
#   --discovery-token-ca-cert-hash sha256:xxx \
#   --control-plane --certificate-key xxx

# WorkerèŠ‚ç‚¹åŠ å…¥
# sudo kubeadm join load-balancer:6443 --token xxx \
#   --discovery-token-ca-cert-hash sha256:xxx
```

### 2.2 ä½¿ç”¨Terraform + EKSï¼ˆAWSï¼‰

```hcl
# terraform/eks-cluster.tf
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.0"

  cluster_name    = "sre-platform-cluster"
  cluster_version = "1.30"

  cluster_endpoint_public_access = true

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent = true
    }
    aws-ebs-csi-driver = {
      most_recent = true
    }
  }

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  # EKS Managed Node Groups
  eks_managed_node_groups = {
    observability = {
      name = "observability-node-group"

      instance_types = ["m5.2xlarge"]
      capacity_type  = "ON_DEMAND"

      min_size     = 3
      max_size     = 10
      desired_size = 3

      labels = {
        workload = "observability"
      }

      taints = [{
        key    = "observability"
        value  = "true"
        effect = "NO_SCHEDULE"
      }]

      tags = {
        Environment = "production"
        Terraform   = "true"
      }
    }

    general = {
      name = "general-node-group"

      instance_types = ["m5.xlarge"]
      capacity_type  = "SPOT"

      min_size     = 2
      max_size     = 20
      desired_size = 5

      labels = {
        workload = "general"
      }
    }
  }

  # Cluster security group
  cluster_security_group_additional_rules = {
    ingress_nodes_ephemeral_ports_tcp = {
      description                = "Nodes on ephemeral ports"
      protocol                   = "tcp"
      from_port                  = 1025
      to_port                    = 65535
      type                       = "ingress"
      source_node_security_group = true
    }
  }

  # Node security group
  node_security_group_additional_rules = {
    ingress_self_all = {
      description = "Node to node all ports/protocols"
      protocol    = "-1"
      from_port   = 0
      to_port     = 0
      type        = "ingress"
      self        = true
    }
    egress_all = {
      description      = "Node all egress"
      protocol         = "-1"
      from_port        = 0
      to_port          = 0
      type             = "egress"
      cidr_blocks      = ["0.0.0.0/0"]
      ipv6_cidr_blocks = ["::/0"]
    }
  }

  tags = {
    Environment = "production"
    Terraform   = "true"
  }
}

# é…ç½®kubectl
resource "null_resource" "kubectl_config" {
  depends_on = [module.eks]

  provisioner "local-exec" {
    command = "aws eks update-kubeconfig --region ${var.region} --name ${module.eks.cluster_name}"
  }
}
```

---

## 3. å¯è§‚æµ‹æ€§æ ˆéƒ¨ç½²

### 3.1 Prometheus Operatoré…ç½®

```yaml
# config/prometheus-values.yaml
prometheus:
  prometheusSpec:
    replicas: 2
    retention: 15d
    retentionSize: "450GB"

    # èµ„æºé…ç½®
    resources:
      requests:
        cpu: 2
        memory: 8Gi
      limits:
        cpu: 4
        memory: 16Gi

    # å­˜å‚¨é…ç½®
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: fast-ssd
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 500Gi

    # Remote Write to VictoriaMetrics
    remoteWrite:
      - url: http://victoria-metrics-cluster-vminsert.observability.svc:8480/insert/0/prometheus/api/v1/write
        queueConfig:
          maxSamplesPerSend: 10000
          maxShards: 30
          capacity: 100000

    # æœåŠ¡å‘ç°é…ç½®
    serviceMonitorSelector: {}
    serviceMonitorNamespaceSelector: {}
    podMonitorSelector: {}
    podMonitorNamespaceSelector: {}

    # é«˜å¯ç”¨é…ç½®
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - prometheus
            topologyKey: kubernetes.io/hostname

# Grafanaé…ç½®
grafana:
  enabled: true
  replicas: 2

  # æŒä¹…åŒ–
  persistence:
    enabled: true
    storageClassName: standard
    size: 10Gi

  # æ•°æ®æº
  additionalDataSources:
    - name: VictoriaMetrics
      type: prometheus
      url: http://victoria-metrics-cluster-vmselect.observability.svc:8481/select/0/prometheus
      access: proxy
      isDefault: true

    - name: Loki
      type: loki
      url: http://loki.observability.svc:3100
      access: proxy

    - name: Tempo
      type: tempo
      url: http://tempo-query-frontend.observability.svc:3100
      access: proxy

  # Dashboardé…ç½®
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default

  # é¢„è£…Dashboard
  dashboards:
    default:
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: VictoriaMetrics
      node-exporter:
        gnetId: 1860
        revision: 27
        datasource: VictoriaMetrics
      prometheus-stats:
        gnetId: 2
        revision: 2
        datasource: VictoriaMetrics

  # èµ„æºé…ç½®
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 1
      memory: 2Gi

# Alertmanageré…ç½®
alertmanager:
  alertmanagerSpec:
    replicas: 3

    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 200m
        memory: 512Mi
```

### 3.2 è‡ªå®šä¹‰ServiceMonitorç¤ºä¾‹

```yaml
# manifests/observability/service-monitors.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: sre-platform-metrics
  namespace: observability
  labels:
    app: sre-platform
spec:
  selector:
    matchLabels:
      app: sre-platform
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
      scheme: http
      # è‡ªå®šä¹‰æ ‡ç­¾
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_name]
          targetLabel: pod
        - sourceLabels: [__meta_kubernetes_namespace]
          targetLabel: namespace
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: node

---
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-mesh-metrics
  namespace: observability
spec:
  selector:
    matchLabels:
      istio: monitor
  podMetricsEndpoints:
    - port: http-envoy-prom
      interval: 15s
      path: /stats/prometheus
      relabelings:
        - sourceLabels: [__meta_kubernetes_pod_container_name]
          action: keep
          regex: istio-proxy
```

### 3.3 Lokié…ç½®

```yaml
# config/loki-values.yaml
loki:
  auth_enabled: false

  server:
    http_listen_port: 3100

  ingester:
    lifecycler:
      ring:
        kvstore:
          store: inmemory
        replication_factor: 3
    chunk_idle_period: 15m
    chunk_retain_period: 30s
    max_chunk_age: 1h

  schema_config:
    configs:
      - from: 2024-01-01
        store: boltdb-shipper
        object_store: s3
        schema: v11
        index:
          prefix: loki_index_
          period: 24h

  storage_config:
    boltdb_shipper:
      active_index_directory: /loki/index
      cache_location: /loki/cache
      shared_store: s3

    aws:
      s3: s3://us-west-2/loki-bucket
      s3forcepathstyle: true

  compactor:
    working_directory: /loki/compactor
    shared_store: s3
    compaction_interval: 10m

  limits_config:
    enforce_metric_name: false
    reject_old_samples: true
    reject_old_samples_max_age: 168h
    ingestion_rate_mb: 50
    ingestion_burst_size_mb: 100

  chunk_store_config:
    max_look_back_period: 720h  # 30 days

  table_manager:
    retention_deletes_enabled: true
    retention_period: 720h

  ruler:
    storage:
      type: local
      local:
        directory: /rules
    rule_path: /tmp/rules
    alertmanager_url: http://kube-prometheus-stack-alertmanager.observability.svc:9093
    ring:
      kvstore:
        store: inmemory
    enable_api: true

# Promtailé…ç½®
promtail:
  enabled: true

  config:
    clients:
      - url: http://loki.observability.svc:3100/loki/api/v1/push
        tenant_id: default

    positions:
      filename: /run/promtail/positions.yaml

    scrape_configs:
      # Kubernetes Podæ—¥å¿—
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod

        pipeline_stages:
          # è§£æJSONæ—¥å¿—
          - json:
              expressions:
                timestamp: timestamp
                level: level
                message: message
                trace_id: trace_id

          # æå–æ ‡ç­¾
          - labels:
              level:
              trace_id:

          # æ—¶é—´æˆ³è§£æ
          - timestamp:
              source: timestamp
              format: RFC3339Nano

        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true

          - source_labels: [__meta_kubernetes_pod_label_app]
            target_label: app

          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace

          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
```

### 3.4 Tempoé…ç½®

```yaml
# config/tempo-values.yaml
tempo:
  replicas: 3

  storage:
    trace:
      backend: s3
      s3:
        bucket: tempo-traces
        endpoint: s3.us-west-2.amazonaws.com
        region: us-west-2

  compactor:
    compaction:
      block_retention: 720h  # 30 days

  distributor:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      jaeger:
        protocols:
          thrift_http:
            endpoint: 0.0.0.0:14268

  ingester:
    trace_idle_period: 10s
    max_block_bytes: 1048576
    max_block_duration: 5m

  querier:
    max_concurrent_queries: 20
    search:
      external_endpoints:
        - http://tempo-query-frontend.observability.svc:3100

  query_frontend:
    search:
      max_duration: 0
      default_result_limit: 20

# OpenTelemetry Collector
opentelemetry-collector:
  enabled: true
  mode: deployment

  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch:
        timeout: 5s
        send_batch_size: 1024

      memory_limiter:
        check_interval: 1s
        limit_mib: 2048

      # é‡‡æ ·
      probabilistic_sampler:
        sampling_percentage: 10  # 10% é‡‡æ ·ç‡

    exporters:
      otlp:
        endpoint: tempo-distributor.observability.svc:4317
        tls:
          insecure: true

      logging:
        loglevel: debug

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, probabilistic_sampler]
          exporters: [otlp, logging]
```

---

## 4. å‘Šè­¦ç³»ç»Ÿé…ç½®

### 4.1 æ ¸å¿ƒå‘Šè­¦è§„åˆ™

```yaml
# manifests/observability/alert-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sre-core-alerts
  namespace: observability
spec:
  groups:
    # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
    - name: service-availability
      interval: 30s
      rules:
        - alert: ServiceDown
          expr: up{job=~".*"} == 0
          for: 5m
          labels:
            severity: critical
            category: availability
          annotations:
            summary: "Service {{ $labels.job }} is down"
            description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 5 minutes"
            runbook_url: "https://wiki.example.com/runbooks/service-down"

        - alert: HighErrorRate
          expr: |
            (
              sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
              /
              sum(rate(http_requests_total[5m])) by (service)
            ) > 0.05
          for: 5m
          labels:
            severity: warning
            category: errors
          annotations:
            summary: "High error rate on {{ $labels.service }}"
            description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

    # å»¶è¿Ÿå‘Šè­¦
    - name: latency
      interval: 30s
      rules:
        - alert: HighLatencyP99
          expr: |
            histogram_quantile(0.99,
              sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
            ) > 1
          for: 10m
          labels:
            severity: warning
            category: latency
          annotations:
            summary: "High P99 latency on {{ $labels.service }}"
            description: "P99 latency is {{ $value }}s (threshold: 1s)"

        - alert: HighLatencyP95
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)
            ) > 0.5
          for: 15m
          labels:
            severity: info
            category: latency
          annotations:
            summary: "Elevated P95 latency on {{ $labels.service }}"
            description: "P95 latency is {{ $value }}s (threshold: 0.5s)"

    # èµ„æºå‘Šè­¦
    - name: resources
      interval: 60s
      rules:
        - alert: HighCPUUsage
          expr: |
            100 - (avg by (instance) (
              rate(node_cpu_seconds_total{mode="idle"}[5m])
            ) * 100) > 80
          for: 15m
          labels:
            severity: warning
            category: resources
          annotations:
            summary: "High CPU usage on {{ $labels.instance }}"
            description: "CPU usage is {{ $value | humanize }}% (threshold: 80%)"

        - alert: HighMemoryUsage
          expr: |
            (1 - (
              node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes
            )) * 100 > 90
          for: 10m
          labels:
            severity: critical
            category: resources
          annotations:
            summary: "High memory usage on {{ $labels.instance }}"
            description: "Memory usage is {{ $value | humanize }}% (threshold: 90%)"

        - alert: DiskSpaceRunningOut
          expr: |
            (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*"}
            / node_filesystem_size_bytes) * 100 < 10
          for: 5m
          labels:
            severity: critical
            category: resources
          annotations:
            summary: "Disk space running out on {{ $labels.instance }}"
            description: "Only {{ $value | humanize }}% space left on {{ $labels.mountpoint }}"

    # Kuberneteså‘Šè­¦
    - name: kubernetes
      interval: 30s
      rules:
        - alert: PodCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total[15m]) > 0
          for: 5m
          labels:
            severity: warning
            category: kubernetes
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            description: "Pod has restarted {{ $value }} times in the last 15 minutes"

        - alert: DeploymentReplicasMismatch
          expr: |
            kube_deployment_spec_replicas
            !=
            kube_deployment_status_replicas_available
          for: 15m
          labels:
            severity: warning
            category: kubernetes
          annotations:
            summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
            description: "Expected replicas: {{ $labels.spec_replicas }}, Available: {{ $value }}"

        - alert: PodNotReady
          expr: |
            sum by (namespace, pod) (
              kube_pod_status_phase{phase!~"Running|Succeeded"}
            ) > 0
          for: 15m
          labels:
            severity: warning
            category: kubernetes
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
            description: "Pod has been in {{ $labels.phase }} phase for more than 15 minutes"
```

### 4.2 Alertmanageré…ç½®

```yaml
# config/alertmanager-config.yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK'

# è·¯ç”±æ ‘
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h

  routes:
    # P0å‘Šè­¦ - ç«‹å³é€šçŸ¥PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      group_wait: 10s
      repeat_interval: 5m
      continue: true

    # P1å‘Šè­¦ - Slack + Email
    - match:
        severity: warning
      receiver: 'team-slack'
      group_wait: 30s
      repeat_interval: 4h

    # ä¸šåŠ¡æ—¶é—´å†…çš„å‘Šè­¦
    - match:
        category: availability
      receiver: 'oncall-team'
      time_intervals:
        - business-hours

    # ç‰¹å®šæœåŠ¡çš„å‘Šè­¦
    - match_re:
        service: payment-service|auth-service
      receiver: 'critical-services-team'
      continue: false

# æŠ‘åˆ¶è§„åˆ™
inhibit_rules:
  # èŠ‚ç‚¹downæ—¶æŠ‘åˆ¶è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰å‘Šè­¦
  - source_match:
      alertname: 'NodeDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']

  # æœåŠ¡downæ—¶æŠ‘åˆ¶é«˜é”™è¯¯ç‡å‘Šè­¦
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'HighErrorRate'
    equal: ['service']

  # Criticalå‘Šè­¦æŠ‘åˆ¶Warningå‘Šè­¦
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']

# æ—¶é—´çª—å£å®šä¹‰
time_intervals:
  - name: business-hours
    time_intervals:
      - times:
          - start_time: '09:00'
            end_time: '18:00'
        weekdays: ['monday:friday']
        location: 'America/New_York'

# æ¥æ”¶å™¨é…ç½®
receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'SRE Alert'
        text: |
          {{ range .Alerts }}
            *Alert:* {{ .Labels.alertname }}
            *Severity:* {{ .Labels.severity }}
            *Description:* {{ .Annotations.description }}
            *Details:*
            {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
            {{ end }}
          {{ end }}

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_KEY'
        description: '{{ .GroupLabels.alertname }} - {{ .GroupLabels.service }}'
        severity: '{{ .CommonLabels.severity }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          num_firing: '{{ .Alerts.Firing | len }}'

  - name: 'team-slack'
    slack_configs:
      - channel: '#sre-team'
        send_resolved: true
        title: '[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
            *Alert:* {{ .Labels.alertname }}
            *Severity:* {{ .Labels.severity }}
            *Service:* {{ .Labels.service }}
            *Summary:* {{ .Annotations.summary }}
            *Description:* {{ .Annotations.description }}
            {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
          {{ end }}

  - name: 'oncall-team'
    email_configs:
      - to: 'oncall@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'password'
        headers:
          Subject: '[{{ .Status }}] {{ .GroupLabels.alertname }}'

  - name: 'critical-services-team'
    webhook_configs:
      - url: 'http://sre-platform.observability.svc/api/v1/alerts/webhook'
        send_resolved: true
```

---

## 5. GitOpsæµæ°´çº¿

### 5.1 ArgoCDåº”ç”¨é…ç½®

```yaml
# argocd/applications/sre-platform.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sre-platform
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/your-org/sre-platform
    targetRevision: main
    path: manifests/sre-platform

    # Helmé…ç½®
    helm:
      valueFiles:
        - values-production.yaml
      parameters:
        - name: replicas
          value: "3"
        - name: image.tag
          value: "v1.2.3"

  destination:
    server: https://kubernetes.default.svc
    namespace: observability

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false

    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true

    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m

  # å¥åº·æ£€æŸ¥
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas

---
# argocd/projects/production.yaml
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: production
  namespace: argocd
spec:
  description: Production environment

  sourceRepos:
    - 'https://github.com/your-org/*'

  destinations:
    - namespace: 'observability'
      server: 'https://kubernetes.default.svc'
    - namespace: 'production'
      server: 'https://kubernetes.default.svc'

  clusterResourceWhitelist:
    - group: ''
      kind: Namespace
    - group: 'rbac.authorization.k8s.io'
      kind: ClusterRole
    - group: 'rbac.authorization.k8s.io'
      kind: ClusterRoleBinding

  namespaceResourceWhitelist:
    - group: '*'
      kind: '*'

  orphanedResources:
    warn: true
```

### 5.2 å¤šç¯å¢ƒç®¡ç†

```
GitOps Repository Structure:
â”œâ”€â”€ base/
â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ configmap.yaml
â”œâ”€â”€ overlays/
â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ replicas.yaml
â”‚   â”‚   â””â”€â”€ resources.yaml
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ replicas.yaml
â”‚   â”‚   â””â”€â”€ resources.yaml
â”‚   â””â”€â”€ production/
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â”œâ”€â”€ replicas.yaml
â”‚       â”œâ”€â”€ resources.yaml
â”‚       â””â”€â”€ autoscaling.yaml
```

```yaml
# overlays/production/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: production

resources:
  - ../../base

replicas:
  - name: sre-platform
    count: 3

images:
  - name: sre-platform
    newTag: v1.2.3

patches:
  - path: resources.yaml
  - path: autoscaling.yaml

configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - ENVIRONMENT=production
      - LOG_LEVEL=info
      - METRICS_ENABLED=true
```

---

## 6. æœåŠ¡æ¥å…¥æŒ‡å—

### 6.1 åº”ç”¨Instrumentation

#### Ruståº”ç”¨æ¥å…¥

```rust
// src/main.rs
use actix_web::{middleware, web, App, HttpServer};
use opentelemetry::{global, sdk::trace as sdktrace, KeyValue};
use opentelemetry_otlp::WithExportConfig;
use prometheus::{Encoder, TextEncoder, register_histogram_vec, register_counter_vec};
use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};

// åˆå§‹åŒ–å¯è§‚æµ‹æ€§
fn init_observability() {
    // 1. é…ç½®OpenTelemetryè¿½è¸ª
    let tracer = opentelemetry_otlp::new_pipeline()
        .tracing()
        .with_exporter(
            opentelemetry_otlp::new_exporter()
                .tonic()
                .with_endpoint("http://otel-collector.observability.svc:4317")
        )
        .with_trace_config(
            sdktrace::config().with_resource(opentelemetry::sdk::Resource::new(vec![
                KeyValue::new("service.name", env!("CARGO_PKG_NAME")),
                KeyValue::new("service.version", env!("CARGO_PKG_VERSION")),
                KeyValue::new("deployment.environment", std::env::var("ENVIRONMENT").unwrap_or_default()),
            ]))
        )
        .install_batch(opentelemetry::runtime::Tokio)
        .expect("Failed to initialize tracer");

    // 2. é…ç½®æ—¥å¿—
    tracing_subscriber::registry()
        .with(tracing_opentelemetry::layer().with_tracer(tracer))
        .with(
            tracing_subscriber::fmt::layer()
                .json()
                .with_current_span(true)
                .with_span_list(true)
        )
        .with(tracing_subscriber::EnvFilter::from_default_env())
        .init();

    // 3. æ³¨å†ŒPrometheusæŒ‡æ ‡
    register_metrics();
}

// å®šä¹‰æŒ‡æ ‡
lazy_static::lazy_static! {
    static ref HTTP_REQUEST_DURATION: prometheus::HistogramVec = register_histogram_vec!(
        "http_request_duration_seconds",
        "HTTP request duration in seconds",
        &["method", "endpoint", "status"]
    ).unwrap();

    static ref HTTP_REQUESTS_TOTAL: prometheus::CounterVec = register_counter_vec!(
        "http_requests_total",
        "Total HTTP requests",
        &["method", "endpoint", "status"]
    ).unwrap();
}

// Metricsç«¯ç‚¹
async fn metrics_handler() -> actix_web::Result<actix_web::HttpResponse> {
    let encoder = TextEncoder::new();
    let metric_families = prometheus::gather();
    let mut buffer = vec![];
    encoder.encode(&metric_families, &mut buffer).unwrap();

    Ok(actix_web::HttpResponse::Ok()
        .content_type("text/plain; version=0.0.4")
        .body(buffer))
}

// ä¸­é—´ä»¶ï¼šè®°å½•è¯·æ±‚æŒ‡æ ‡
async fn metrics_middleware(
    req: actix_web::HttpRequest,
    srv: &mut actix_web::dev::ServiceRequest,
) -> Result<actix_web::dev::ServiceResponse, actix_web::Error> {
    let start = std::time::Instant::now();
    let method = req.method().to_string();
    let path = req.path().to_string();

    let res = srv.call().await?;

    let duration = start.elapsed().as_secs_f64();
    let status = res.status().as_u16().to_string();

    HTTP_REQUEST_DURATION
        .with_label_values(&[&method, &path, &status])
        .observe(duration);

    HTTP_REQUESTS_TOTAL
        .with_label_values(&[&method, &path, &status])
        .inc();

    Ok(res)
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    init_observability();

    HttpServer::new(|| {
        App::new()
            .wrap(middleware::Logger::default())
            .wrap_fn(metrics_middleware)
            .route("/metrics", web::get().to(metrics_handler))
            .route("/health", web::get().to(health_check))
            .service(/* your routes */)
    })
    .bind(("0.0.0.0", 8080))?
    .run()
    .await
}
```

#### Javaåº”ç”¨æ¥å…¥

```java
// Application.java
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.prometheus.PrometheusConfig;
import io.micrometer.prometheus.PrometheusMeterRegistry;
import io.opentelemetry.api.OpenTelemetry;
import io.opentelemetry.sdk.autoconfigure.AutoConfiguredOpenTelemetrySdk;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

@SpringBootApplication
public class Application {

    public static void main(String[] args) {
        // 1. åˆå§‹åŒ–OpenTelemetry (é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®)
        System.setProperty("otel.service.name", "my-service");
        System.setProperty("otel.exporter.otlp.endpoint", "http://otel-collector:4317");
        System.setProperty("otel.metrics.exporter", "otlp");
        System.setProperty("otel.logs.exporter", "otlp");

        OpenTelemetry openTelemetry = AutoConfiguredOpenTelemetrySdk.initialize()
                .getOpenTelemetrySdk();

        SpringApplication.run(Application.class, args);
    }

    @Bean
    public MeterRegistry meterRegistry() {
        return new PrometheusMeterRegistry(PrometheusConfig.DEFAULT);
    }
}

// MetricsController.java
import io.micrometer.prometheus.PrometheusMeterRegistry;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class MetricsController {

    private final PrometheusMeterRegistry registry;

    public MetricsController(PrometheusMeterRegistry registry) {
        this.registry = registry;
    }

    @GetMapping("/actuator/prometheus")
    public String prometheus() {
        return registry.scrape();
    }
}

// application.yml
management:
  endpoints:
    web:
      exposure:
        include: health,prometheus,metrics
  metrics:
    tags:
      application: ${spring.application.name}
      environment: ${ENVIRONMENT:dev}
    distribution:
      percentiles-histogram:
        http.server.requests: true
```

### 6.2 Kuberneteséƒ¨ç½²é…ç½®

```yaml
# manifests/service-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: production
  labels:
    app: my-service
    version: v1
  annotations:
    # Prometheusé‡‡é›†é…ç½®
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
spec:
  selector:
    app: my-service
  ports:
    - name: http
      port: 80
      targetPort: 8080
    - name: metrics
      port: 9090
      targetPort: 8080

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
  namespace: production
  labels:
    app: my-service
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-service
      version: v1

  template:
    metadata:
      labels:
        app: my-service
        version: v1
      annotations:
        # Istio sidecaræ³¨å…¥
        sidecar.istio.io/inject: "true"
        # Prometheusé‡‡é›†
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"

    spec:
      # æœåŠ¡è´¦å·
      serviceAccountName: my-service

      # å®¹å™¨é…ç½®
      containers:
        - name: app
          image: your-registry/my-service:v1.2.3

          ports:
            - containerPort: 8080
              name: http
              protocol: TCP

          # ç¯å¢ƒå˜é‡
          env:
            - name: ENVIRONMENT
              value: "production"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: "http://otel-collector.observability.svc:4317"
            - name: LOG_LEVEL
              value: "info"

          # èµ„æºé™åˆ¶
          resources:
            requests:
              cpu: 500m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 1Gi

          # å¥åº·æ£€æŸ¥
          livenessProbe:
            httpGet:
              path: /health/live
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /health/ready
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 3

          # å¯åŠ¨æ¢é’ˆ
          startupProbe:
            httpGet:
              path: /health/startup
              port: 8080
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 30

      # Podåäº²å’Œæ€§ï¼ˆé«˜å¯ç”¨ï¼‰
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - my-service
                topologyKey: kubernetes.io/hostname

---
# HPAé…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-service
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
      selectPolicy: Max
```

---

## 7. æ•…éšœæ’æŸ¥

### 7.1 å¸¸è§é—®é¢˜æ’æŸ¥æµç¨‹

#### 7.1.1 æœåŠ¡ä¸å¯ç”¨

```bash
# 1. æ£€æŸ¥PodçŠ¶æ€
kubectl get pods -n production -l app=my-service
kubectl describe pod -n production <pod-name>

# 2. æŸ¥çœ‹æ—¥å¿—
kubectl logs -n production <pod-name> --tail=100 --follow

# 3. æ£€æŸ¥äº‹ä»¶
kubectl get events -n production --sort-by='.lastTimestamp'

# 4. æ£€æŸ¥æœåŠ¡ç«¯ç‚¹
kubectl get endpoints -n production my-service

# 5. æµ‹è¯•æœåŠ¡è¿é€šæ€§
kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- \
  curl http://my-service.production.svc/health

# 6. æŸ¥çœ‹Grafanaç›‘æ§
# è®¿é—®: http://grafana.example.com/d/service-dashboard
```

#### 7.1.2 é«˜å»¶è¿Ÿæ’æŸ¥

```bash
# 1. æŸ¥çœ‹Tempoè¿½è¸ª
# åœ¨Grafana Exploreä¸­æ‰§è¡Œ:
# Query: { service.name="my-service" } | duration > 1s

# 2. æ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—
kubectl logs -n production <pod-name> | grep "duration_ms"

# 3. åˆ†æPrometheusæŒ‡æ ‡
# PromQLæŸ¥è¯¢:
histogram_quantile(0.99,
  sum(rate(http_request_duration_seconds_bucket{service="my-service"}[5m])) by (le, endpoint)
)

# 4. æ£€æŸ¥æ•°æ®åº“è¿æ¥
kubectl exec -it -n production <pod-name> -- netstat -an | grep ESTABLISHED
```

### 7.2 è°ƒè¯•å·¥å…·é›†

```bash
# debug-toolkit.sh
#!/bin/bash

# éƒ¨ç½²è°ƒè¯•Pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: debug-toolkit
  namespace: production
spec:
  containers:
    - name: toolkit
      image: nicolaka/netshoot
      command: ["/bin/bash"]
      args: ["-c", "sleep 3600"]
      securityContext:
        capabilities:
          add: ["NET_ADMIN", "SYS_PTRACE"]
EOF

# ç­‰å¾…Podå°±ç»ª
kubectl wait --for=condition=ready pod/debug-toolkit -n production --timeout=60s

echo "Debug toolkit deployed. Usage:"
echo "  kubectl exec -it -n production debug-toolkit -- bash"
echo ""
echo "Available tools:"
echo "  - curl, wget"
echo "  - tcpdump"
echo "  - nslookup, dig"
echo "  - netstat, ss"
echo "  - strace"
echo "  - iperf3"
```

---

## æ€»ç»“

æœ¬éƒ¨ç½²æŒ‡å—æä¾›äº†SREå¹³å°ä»é›¶åˆ°ä¸€çš„å®Œæ•´å®æ–½è·¯å¾„ï¼š

1. **åŸºç¡€è®¾æ–½**: Kubernetesé›†ç¾¤æ­å»º
2. **å¯è§‚æµ‹æ€§**: Prometheusã€Lokiã€Tempoå…¨æ ˆéƒ¨ç½²
3. **å‘Šè­¦ç³»ç»Ÿ**: å¤šå±‚çº§å‘Šè­¦é…ç½®ä¸é™å™ªç­–ç•¥
4. **è‡ªåŠ¨åŒ–**: GitOpsæŒç»­éƒ¨ç½²æµæ°´çº¿
5. **æœåŠ¡æ¥å…¥**: å¤šè¯­è¨€åº”ç”¨Instrumentation
6. **æ•…éšœæ’æŸ¥**: ç³»ç»ŸåŒ–æ’éšœæµç¨‹ä¸å·¥å…·

é€šè¿‡éµå¾ªæœ¬æŒ‡å—ï¼Œå¯åœ¨2-3å‘¨å†…æ­å»ºå®Œæ•´çš„ä¼ä¸šçº§SREå¹³å°ï¼Œå®ç°ï¼š
- âœ… ç«¯åˆ°ç«¯å¯è§‚æµ‹æ€§
- âœ… æ™ºèƒ½å‘Šè­¦ä¸è‡ªæ„ˆ
- âœ… å£°æ˜å¼GitOpséƒ¨ç½²
- âœ… 99.99%æœåŠ¡å¯ç”¨æ€§

**ä¸‹ä¸€æ­¥**:
- æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚è°ƒæ•´é…ç½®å‚æ•°
- å»ºç«‹SLO/SLIä½“ç³»
- å®šæœŸè¿›è¡Œæ··æ²Œå·¥ç¨‹æ¼”ç»ƒ
- æŒç»­ä¼˜åŒ–æˆæœ¬ä¸æ€§èƒ½

ç¥éƒ¨ç½²é¡ºåˆ©ï¼ğŸš€
