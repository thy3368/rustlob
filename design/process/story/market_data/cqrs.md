# CQRS架构下的TPS/QPS分析

## 概念定义

### TPS (Transactions Per Second)
- **定义**: 每秒事务处理数，衡量**写操作**的吞吐量
- **CEX场景**: 订单提交、订单取消、账户充提等修改状态的操作
- **特点**:
  - 需要保证ACID事务特性
  - 需要持久化到数据库
  - 需要发布领域事件
  - 时延敏感（微秒级）

### QPS (Queries Per Second)
- **定义**: 每秒查询数，衡量**读操作**的吞吐量
- **CEX场景**: 查询订单、查询余额、查询成交历史、获取市场行情
- **特点**:
  - 不修改状态
  - 可以从缓存/读副本获取
  - 时延相对宽松（毫秒级）
  - 流量通常远大于写操作

---

## CQRS模式

### Command Query Responsibility Segregation
**核心思想**: 读写分离，命令（写）和查询（读）使用不同的模型和数据存储

```
写路径（Command Side - TPS）          读路径（Query Side - QPS）
        ↓                                    ↑
   限价单/撤单/充提                    查询订单/余额/行情
        ↓                                    ↑
  写模型（Write Model）              读模型（Read Model）
        ↓                                    ↑
   领域逻辑验证                        简单的数据投影
        ↓                                    ↑
 持久化到主库（PostgreSQL）          从读库/缓存读取
        ↓                                    ↑
   发布领域事件 ────────────→ 更新读模型（异步）
        ↓
   返回命令结果
```

---

## CEX系统中的TPS/QPS特征

### 1. 读写比例

| 系统类型 | 读写比 | TPS | QPS | 说明 |
|---------|-------|-----|-----|------|
| 现货交易 | 100:1 | 10万 | 1000万 | 用户频繁查询订单状态、市场深度 |
| 合约交易 | 200:1 | 5万 | 1000万 | 仓位查询、盈亏计算查询量大 |
| 账户系统 | 50:1 | 1万 | 50万 | 余额查询频繁 |
| 市场数据 | ∞:1 | 0 | 5000万 | 纯读场景：行情、K线、深度 |

**结论**: QPS通常是TPS的100-1000倍

**市场数据特殊性**：
- 完全是查询场景，无写操作（TPS=0）
- QPS极高，是订单查询的10-100倍
- 实时性要求高（< 100ms延迟）
- 数据量大但更新频率可预测

### 2. 性能瓶颈分析

#### 写路径（TPS瓶颈）
- **单点瓶颈**: 撮合引擎必须单线程保证顺序性
- **ACID约束**: 需要保证事务一致性
- **持久化开销**: 每笔订单需要写入数据库
- **事件发布**: 需要发送Kafka消息

**优化策略**:
- 批量持久化（牺牲部分实时性）
- 内存预分配（避免动态分配）
- 异步事件发布（不阻塞主流程）
- 无锁数据结构

#### 读路径（QPS瓶颈）
- **数据库连接数**: 读库连接池可能耗尽
- **查询复杂度**: 复杂JOIN查询导致慢查询
- **热点数据**: 某些订单/用户被高频查询

**优化策略**:
- Redis缓存热点数据
- 读写分离（主从复制）
- 读副本横向扩展
- CDN加速静态数据
- GraphQL减少过度获取

---

## CQRS在CEX中的实现

### 1. 命令侧（Write Side - TPS优化）

#### 1.1 命令处理流程
```
Client Request
    ↓
幂等性检查（Nonce去重）
    ↓
领域模型验证（Order.validate()）
    ↓
撮合引擎处理（单线程保证顺序）
    ↓
批量写入PostgreSQL（每100ms一批）
    ↓
异步发布事件到Kafka
    ↓
返回CommandResponse
```

#### 1.2 写模型特点
- **聚合根**: Order、Account、Position
- **业务规则**: 余额检查、风险控制、状态机转换
- **事务边界**: 单个聚合根内保证一致性
- **最终一致性**: 跨聚合根通过事件最终一致

#### 1.3 TPS性能指标
| 场景 | 目标TPS | P99时延 |
|------|---------|---------|
| 限价单提交 | 10万 | < 100μs |
| 市价单提交 | 5万 | < 50μs |
| 订单取消 | 20万 | < 50μs |
| 批量取消 | 1万 | < 1ms |

### 2. 查询侧（Read Side - QPS优化）

#### 2.1 查询处理流程
```
Client Request
    ↓
查询Redis缓存（热点数据）
    ↓ (Cache Miss)
查询PostgreSQL读副本
    ↓
写入Redis（设置TTL）
    ↓
返回QueryResult
```

#### 2.2 读模型设计
```
读模型1: OrderView（订单视图）
- 目的: 快速查询订单详情
- 存储: Redis Hash
- 更新: 监听OrderCreatedEvent, OrderUpdatedEvent
- TTL: 24小时

读模型2: OpenOrdersView（活跃订单视图）
- 目的: 查询用户活跃订单列表
- 存储: Redis Sorted Set (score=timestamp)
- 更新: 订单创建时添加，完成/取消时移除
- TTL: 永久（直到订单完成）

读模型3: TradeHistoryView（成交历史）
- 目的: 查询用户成交记录
- 存储: PostgreSQL (TimescaleDB超表)
- 更新: 监听TradeCreatedEvent
- 查询: 支持时间范围、分页

读模型4: MarketDepthView（市场深度）
- 目的: 实时订单簿快照
- 存储: Redis Sorted Set (买卖各一个)
- 更新: 订单变化时增量更新
- 推送: WebSocket广播给订阅用户

读模型5: TickerView（实时行情）
- 目的: 最新成交价、24h统计
- 存储: Redis Hash (key: symbol, fields: last_price, volume_24h, high_24h, low_24h)
- 更新: 每次成交后更新
- 推送: WebSocket实时推送 + HTTP轮询
- TTL: 永久（持续更新）

读模型6: KLineView（K线数据）
- 目的: 分钟/小时/日K线查询
- 存储: TimescaleDB (时序优化) + Redis (热点K线缓存)
- 更新: 时间窗口聚合（1m/5m/15m/1h/4h/1d）
- 查询: 支持时间范围、分页
- TTL: Redis缓存最近24小时

读模型7: RecentTradesView（最新成交）
- 目的: 显示最近N笔成交记录
- 存储: Redis List (LPUSH新成交，LTRIM保留最近500笔)
- 更新: 每次成交后推入
- 推送: WebSocket实时推送
- TTL: 1小时
```

#### 2.3 QPS性能指标
| 场景 | 目标QPS | P99时延 |
|------|---------|---------|
| 查询活跃订单 | 50万 | < 5ms |
| 查询订单详情 | 100万 | < 2ms |
| 查询成交历史 | 10万 | < 50ms |
| 获取市场深度 | 500万 | < 1ms |
| 获取实时行情（Ticker） | 1000万 | < 500μs |
| 获取K线数据 | 200万 | < 10ms |
| 获取最新成交 | 800万 | < 1ms |

### 3. 最终一致性保证

#### 3.1 事件驱动更新
```
写操作完成
    ↓
发布领域事件到Kafka
    ↓
多个读模型Consumer并行消费
    ↓
Consumer 1: 更新Redis缓存
Consumer 2: 更新PostgreSQL读库
Consumer 3: 推送WebSocket通知
Consumer 4: 更新统计指标
```

#### 3.2 一致性窗口
- **理想情况**: < 100ms（Kafka延迟 + 更新时间）
- **最坏情况**: < 1s（Kafka重试 + 数据库写入）
- **可接受**: 用户能容忍短暂的延迟（如余额显示）

#### 3.3 一致性监控
```
写操作时间戳（Command Timestamp）
    vs
读模型更新时间戳（View Update Timestamp）

告警条件: 时间差 > 5秒
```

---

## 数据流图

### 完整数据流
```
┌─────────────────────────────────────────────────────┐
│                  API Gateway                         │
│  (Rate Limiter + Auth + Router)                     │
└────────┬──────────────────────────────┬─────────────┘
         │                               │
    写请求（TPS）                    读请求（QPS）
         ↓                               ↓
┌────────▼────────┐              ┌──────▼──────────┐
│ Command Handler │              │ Query Handler   │
│ (Write Model)   │              │ (Read Model)    │
└────────┬────────┘              └──────┬──────────┘
         │                               │
    领域逻辑处理                    查询优化路径
         ↓                               ↓
┌────────▼────────┐              ┌──────▼──────────┐
│  PostgreSQL     │              │  Redis Cache    │
│  (Write Master) │              │  (Read Cache)   │
└────────┬────────┘              └──────┬──────────┘
         │                               ↑
    发布事件                            ↑
         ↓                               ↑
┌────────▼────────┐                     ↑
│  Kafka Topics   │─────────────────────┘
│  (Event Store)  │    异步更新读模型
└─────────────────┘
```

---

## 性能优化策略

### 1. 写路径优化（提升TPS）

#### 1.1 批量处理
```
单笔处理: 10万 TPS (每笔10μs)
    ↓
批量持久化 (100笔/批)
    ↓
批量TPS: 100万 (每批1ms, 每笔10μs撮合 + 分摊1μs写入)
```

#### 1.2 异步事件发布
```
同步发布: TPS受限于Kafka延迟（~5ms）
    ↓
异步发布: 撮合完成立即返回，后台线程发送事件
    ↓
TPS提升: 10倍+
```

#### 1.3 内存预分配
- 订单对象池（避免动态分配）
- 预分配的环形缓冲区
- 固定大小的事件队列

### 2. 读路径优化（提升QPS）

#### 2.1 多级缓存
```
L1: 应用内存缓存（Caffeine/LRU）
    ↓ (Miss)
L2: Redis集群缓存
    ↓ (Miss)
L3: PostgreSQL读副本
    ↓
回写到L2, L1
```

#### 2.2 读副本扩展
```
1主 + 3从（异步复制）
    ↓
读流量分散到3个从库
    ↓
QPS提升: 3倍+（受限于复制延迟）
```

#### 2.3 预计算与物化视图
```
实时计算: 每次查询JOIN多张表（慢）
    ↓
预计算: 事件触发时更新物化视图
    ↓
查询: 直接读取预计算结果（快）
```

---

## 容量规划

### 1. 写容量（TPS）

#### 单交易对撮合引擎
- **硬件**: 1核心独占（3.5GHz）
- **理论TPS**: 35万（假设每笔100个CPU周期）
- **实际TPS**: 10万（考虑内存访问、事件发布）

#### 横向扩展
```
1个交易对 → 10万 TPS
10个交易对（分片部署） → 100万 TPS
100个交易对 → 1000万 TPS
```

### 2. 读容量（QPS）

#### Redis集群
- **硬件**: 3主 + 3从（64GB内存）
- **单节点QPS**: 10万（简单GET）
- **集群QPS**: 60万（分片）

#### PostgreSQL读副本
- **硬件**: 1主 + 5从（SSD存储）
- **单节点QPS**: 5万（复杂查询）
- **总QPS**: 25万

#### 总读容量
```
Redis缓存命中率: 95%
    ↓
95% × 60万（Redis）+ 5% × 25万（PostgreSQL）
    ↓
总QPS: ~58万
```

---

## 监控指标

### 1. TPS监控
```
指标:
- order_submit_tps: 订单提交速率
- order_match_tps: 撮合成交速率
- order_cancel_tps: 订单取消速率

告警:
- TPS突然下降50% → 撮合引擎故障
- TPS持续超过阈值 → 需要扩容
```

### 2. QPS监控
```
指标:
- query_open_orders_qps: 活跃订单查询速率
- query_order_detail_qps: 订单详情查询速率
- query_trade_history_qps: 成交历史查询速率

告警:
- QPS超过容量80% → 预警扩容
- 缓存命中率 < 90% → 缓存策略调整
```

### 3. 一致性监控
```
指标:
- event_lag_seconds: 事件消费延迟
- read_model_lag_seconds: 读模型更新延迟

告警:
- 延迟 > 5秒 → 数据不一致风险
- 延迟 > 60秒 → 严重故障
```

---

## 架构演进路径

### Phase 1: 单体架构（初期）
```
TPS: 1万，QPS: 10万
架构: 单库读写，无CQRS
适用: MVP验证
```

### Phase 2: 读写分离（成长期）
```
TPS: 5万，QPS: 50万
架构: 主从复制，读写分离
优化: Redis缓存热点数据
```

### Phase 3: CQRS架构（成熟期）
```
TPS: 10万+，QPS: 500万+
架构: 完整CQRS，事件驱动
优化: 多级缓存，读副本扩展，分片部署
```

### Phase 4: 全球化部署（扩张期）
```
TPS: 100万+，QPS: 5000万+
架构: 多地域部署，边缘计算
优化: CDN加速，就近路由，异地多活
```

---

## 总结

### CQRS核心价值
1. **性能隔离**: 读写操作互不影响，分别优化
2. **扩展独立**: 读写分别扩容，适应不同负载
3. **模型分离**: 写模型聚焦业务规则，读模型聚焦查询性能
4. **最终一致**: 通过事件驱动实现跨聚合根的数据同步

### TPS/QPS关系
- **TPS决定系统容量**: 能处理多少笔交易
- **QPS决定用户体验**: 查询是否流畅
- **CQRS是平衡手段**: 用最终一致性换取更高的吞吐量

### 实施要点
1. 根据读写比例决定是否采用CQRS（读写比 > 10:1 推荐）
2. 评估最终一致性的业务可接受度
3. 监控事件消费延迟，确保一致性窗口可控
4. 分阶段演进，避免过度设计

---

**文档版本**: v1.0.0
**最后更新**: 2025-01-08
**参考**: `/Users/hongyaotang/src/rustlob/app/design/process/spot.md`
